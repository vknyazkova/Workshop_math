{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a664fe8e",
   "metadata": {},
   "source": [
    "# Создание базы данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd6d509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "565d4f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_queries = ['''\n",
    "# CREATE TABLE \"texts\" ( \n",
    "# \t\"id\"\tINTEGER,\n",
    "# \t\"name\"\tTEXT UNIQUE,\n",
    "# \tPRIMARY KEY(\"id\" AUTOINCREMENT)\n",
    "# );''', \n",
    "# '''\n",
    "# CREATE TABLE \"sents\" (\n",
    "# \t\"id\"\tINTEGER,\n",
    "# \t\"text_id\"\tINTEGER,\n",
    "# \t\"sent\"\tTEXT,\n",
    "# \t\"lemmatized\"\tTEXT,\n",
    "# \t\"pos_in_text\"\tINTEGER,\n",
    "# \tFOREIGN KEY(\"text_id\") REFERENCES \"texts\"(\"id\"),\n",
    "# \tPRIMARY KEY(\"id\" AUTOINCREMENT)\n",
    "# );''', \n",
    "\n",
    "# '''\n",
    "# CREATE TABLE \"govern_models\" (\n",
    "# \t\"id\"\tINTEGER,\n",
    "# \t\"model\"\tTEXT UNIQUE,\n",
    "# \tPRIMARY KEY(\"id\" AUTOINCREMENT)\n",
    "# );\n",
    "# ''',\n",
    "\n",
    "# '''\n",
    "# CREATE TABLE \"math_imgs\" (\n",
    "# \t\"id\"\tINTEGER,\n",
    "# \t\"math_tagname\"\tTEXT,\n",
    "# \t\"filename\"\tTEXT,\n",
    "# \tPRIMARY KEY(\"id\" AUTOINCREMENT)\n",
    "# );\n",
    "# ''',\n",
    "\n",
    "# '''\n",
    "# CREATE TABLE \"math_tags\" (\n",
    "# \t\"id\"\tINTEGER,\n",
    "# \t\"feature\"\tTEXT,\n",
    "# \tPRIMARY KEY(\"id\" AUTOINCREMENT)\n",
    "# );\n",
    "# ''',\n",
    "\n",
    "# '''\n",
    "# CREATE TABLE \"lemmas\" (\n",
    "# \t\"id\"\tINTEGER,\n",
    "# \t\"name\"\tTEXT UNIQUE,\n",
    "# \tPRIMARY KEY(\"id\" AUTOINCREMENT)\n",
    "# );\n",
    "# ''',\n",
    "\n",
    "# '''\n",
    "# CREATE TABLE \"pos\" (\n",
    "# \t\"id\"\tINTEGER,\n",
    "# \t\"name\"\tTEXT,\n",
    "# \tPRIMARY KEY(\"id\" AUTOINCREMENT)\n",
    "# );\n",
    "# ''',\n",
    "\n",
    "# '''\n",
    "# CREATE TABLE \"deprels\" (\n",
    "# \t\"id\"\tINTEGER,\n",
    "# \t\"name\"\tINTEGER,\n",
    "# \tPRIMARY KEY(\"id\" AUTOINCREMENT)\n",
    "# );\n",
    "# ''',\n",
    "\n",
    "# '''\n",
    "# CREATE TABLE \"tokens\" (\n",
    "# \t\"id\"\tINTEGER,\n",
    "# \t\"sent_id\"\tINTEGER,\n",
    "# \t\"word_in_sent\"\tINTEGER,\n",
    "# \t\"token\"\tTEXT,\n",
    "# \tFOREIGN KEY(\"sent_id\") REFERENCES \"sents\"(\"id\") ON DELETE RESTRICT,\n",
    "# \tPRIMARY KEY(\"id\" AUTOINCREMENT)\n",
    "# );\n",
    "# ''', \n",
    "\n",
    "# '''\n",
    "# CREATE TABLE \"math_annotation\" (\n",
    "# \t\"id\"\tINTEGER,\n",
    "# \t\"id_token_start\"\tINTEGER,\n",
    "# \t\"id_token_end\"\tINTEGER,\n",
    "# \t\"feature_id\"\tINTEGER,\n",
    "# \t\"govern_model_id\"\tINTEGER,\n",
    "# \tFOREIGN KEY(\"feature_id\") REFERENCES \"math_tags\"(\"id\"),\n",
    "# \tFOREIGN KEY(\"govern_model_id\") REFERENCES \"govern_models\"(\"id\"),\n",
    "# \tFOREIGN KEY(\"id_token_start\") REFERENCES \"tokens\"(\"id\"),\n",
    "# \tFOREIGN KEY(\"id_token_end\") REFERENCES \"tokens\"(\"id\"),\n",
    "# \tPRIMARY KEY(\"id\" AUTOINCREMENT)\n",
    "# );\n",
    "# ''',\n",
    "\n",
    "# '''\n",
    "# CREATE TABLE \"grammar_annotation\" (\n",
    "# \t\"token_id\"\tINTEGER,\n",
    "# \t\"lemma_id\"\tINTEGER,\n",
    "# \t\"pos_id\"\tINTEGER,\n",
    "# \t\"deprel_id\"\tINTEGER,\n",
    "# \t\"head_id\"\tINTEGER,\n",
    "# \tFOREIGN KEY(\"lemma_id\") REFERENCES \"lemmas\"(\"id\"),\n",
    "# \tFOREIGN KEY(\"token_id\") REFERENCES \"tokens\"(\"id\"),\n",
    "# \tFOREIGN KEY(\"deprel_id\") REFERENCES \"deprels\"(\"id\"),\n",
    "# \tFOREIGN KEY(\"head_id\") REFERENCES \"tokens\"(\"id\"),\n",
    "# \tFOREIGN KEY(\"pos_id\") REFERENCES \"pos\"(\"id\"),\n",
    "# \tPRIMARY KEY(\"token_id\")\n",
    "# );\n",
    "# '''\n",
    "#                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93703882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_path = 'math_corpus_database.db'\n",
    "# conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "# cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "92f17f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for q in create_queries:\n",
    "#     cur.execute(q)\n",
    "#     conn.commit()\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b514830d",
   "metadata": {},
   "source": [
    "# Парсинг и заполнение БД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71d28c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (3.5.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.0.11)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\program files\\conda\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (8.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.9.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\program files\\conda\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.4.4)\n",
      "Requirement already satisfied: jinja2 in c:\\program files\\conda\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\program files\\conda\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\program files\\conda\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.0.7)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\program files\\conda\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\program files\\conda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\program files\\conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.12)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
      "Requirement already satisfied: colorama in c:\\program files\\conda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\program files\\conda\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\program files\\conda\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce9d6760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting ru-core-news-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.5.0/ru_core_news_sm-3.5.0-py3-none-any.whl (15.3 MB)\n",
      "     ---------------------------------------- 15.3/15.3 MB 9.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pymorphy3>=1.0.0 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from ru-core-news-sm==3.5.0) (1.2.0)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from ru-core-news-sm==3.5.0) (3.5.0)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.5.0) (0.7.2)\n",
      "Requirement already satisfied: docopt>=0.6 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.5.0) (0.6.2)\n",
      "Requirement already satisfied: pymorphy3-dicts-ru in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.5.0) (2.4.417150.4580142)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\program files\\conda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.27.1)\n",
      "Requirement already satisfied: jinja2 in c:\\program files\\conda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.11.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\program files\\conda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.9.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.0.11)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (65.5.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\program files\\conda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.21.5)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\program files\\conda\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (4.64.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.4.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.4.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\program files\\conda\\lib\\site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\program files\\conda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (4.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\program files\\conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\kra$0t04ka\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.7.8)\n",
      "Requirement already satisfied: colorama in c:\\program files\\conda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\program files\\conda\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\program files\\conda\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.0.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('ru_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ebd7b8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in c:\\program files\\conda\\lib\\site-packages (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\program files\\conda\\lib\\site-packages (from tqdm) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5e65c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"ru_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7772d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = '''<?xml version='1.0' encoding='utf-8'?>\n",
    "<document>\n",
    "<header>\n",
    "<textfile>Texts/limit_test.txt</textfile>\n",
    "<lang>english</lang></header><body>\n",
    "<segment id='45' features='math_tags' state='active' subcorp='' filename=''>И дальше мы говорим, что вот какое-то число <segment id='164' features='math_tags;term' state='active' subcorp='' filename=''><segment id='163' features='math_tags;term;var' state='active' subcorp='' filename=''>а</segment> большое</segment> является <segment id='165' features='math_tags;function' state='active' subcorp='' filename=''><segment id='166' features='math_tags;term' state='active' subcorp='' filename=''>пределом </segment><segment id='167' features='math_tags;visual' state='active' subcorp='' filename=''>при <segment id='168' features='math_tags;term' state='active' subcorp='' filename=''>икс </segment></segment><segment id='169' features='math_tags;visual' state='active' subcorp='' filename=''>стремящемся к <segment id='170' features='math_tags;term' state='active' subcorp='' filename=''><segment id='171' features='math_tags;term;var' state='active' subcorp='' filename=''>а</segment> маленькому</segment></segment></segment>, если, какое бы малое <segment id='173' features='math_tags;term' state='active' subcorp='' filename=''>эпсилон</segment> вы вот тут не взяли… Да, потому что <segment id='174' features='math_tags;term' state='active' subcorp='' filename=''>эпсилон</segment>, оно про <segment id='175' features='math_tags;term' state='active' subcorp='' filename=''>а большое</segment>.</segment>\n",
    "<segment id='46' features='math_tags' state='active' subcorp='' filename=''>Какое бы маленькое <segment id='172' features='math_tags;term' state='active' subcorp='' filename=''>эпсилон</segment> будут не взяли, существует такое <segment id='176' features='math_tags;term' state='active' subcorp='' filename=''>дельта</segment>, что все <segment id='177' features='math_tags;term' state='active' subcorp='' filename=''>иксы</segment>, которые лежат вот здесь, попадают вот сюда.</segment>\n",
    "</body>\n",
    "</document>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "750db482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для красивого печатания\n",
    "def bolded_print(sent_string, seg_span):\n",
    "    start, end = seg_span\n",
    "    print(sent_string[:start] + '\\033[1m' + sent_string[start:end] + '\\033[0m' + sent_string[end:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "926e4c2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "\u001b[1m\u001b[0mИ дальше мы говорим, что вот какое-то число а большое является пределом при икс стремящемся к а маленькому, если, какое бы малое эпсилон вы вот тут не взяли…\n",
      "\n",
      "164\n",
      "И дальше мы говорим, что вот какое-то число \u001b[1mа большое\u001b[0m является пределом при икс стремящемся к а маленькому, если, какое бы малое эпсилон вы вот тут не взяли…\n",
      "\n",
      "163\n",
      "И дальше мы говорим, что вот какое-то число \u001b[1mа\u001b[0m большое является пределом при икс стремящемся к а маленькому, если, какое бы малое эпсилон вы вот тут не взяли…\n",
      "\n",
      "165\n",
      "И дальше мы говорим, что вот какое-то число а большое является \u001b[1mпределом при икс стремящемся к а маленькому\u001b[0m, если, какое бы малое эпсилон вы вот тут не взяли…\n",
      "\n",
      "166\n",
      "И дальше мы говорим, что вот какое-то число а большое является \u001b[1mпределом\u001b[0m при икс стремящемся к а маленькому, если, какое бы малое эпсилон вы вот тут не взяли…\n",
      "\n",
      "167\n",
      "И дальше мы говорим, что вот какое-то число а большое является пределом \u001b[1mпри икс\u001b[0m стремящемся к а маленькому, если, какое бы малое эпсилон вы вот тут не взяли…\n",
      "\n",
      "168\n",
      "И дальше мы говорим, что вот какое-то число а большое является пределом при \u001b[1mикс\u001b[0m стремящемся к а маленькому, если, какое бы малое эпсилон вы вот тут не взяли…\n",
      "\n",
      "169\n",
      "И дальше мы говорим, что вот какое-то число а большое является пределом при икс \u001b[1mстремящемся к а маленькому\u001b[0m, если, какое бы малое эпсилон вы вот тут не взяли…\n",
      "\n",
      "170\n",
      "И дальше мы говорим, что вот какое-то число а большое является пределом при икс стремящемся к \u001b[1mа маленькому\u001b[0m, если, какое бы малое эпсилон вы вот тут не взяли…\n",
      "\n",
      "171\n",
      "И дальше мы говорим, что вот какое-то число а большое является пределом при икс стремящемся к \u001b[1mа\u001b[0m маленькому, если, какое бы малое эпсилон вы вот тут не взяли…\n",
      "\n",
      "173\n",
      "И дальше мы говорим, что вот какое-то число а большое является пределом при икс стремящемся к а маленькому, если, какое бы малое \u001b[1mэпсилон\u001b[0m вы вот тут не взяли…\n",
      "\n",
      "174\n",
      "Да, потому что \u001b[1mэпсилон\u001b[0m, оно про а большое.\n",
      "\n",
      "175\n",
      "Да, потому что эпсилон, оно про \u001b[1mа большое\u001b[0m.\n",
      "\n",
      "46\n",
      "\u001b[1m\u001b[0mКакое бы маленькое эпсилон будут не взяли, существует такое дельта, что все иксы, которые лежат вот здесь, попадают вот сюда.\n",
      "\n",
      "172\n",
      "Какое бы маленькое \u001b[1mэпсилон\u001b[0m будут не взяли, существует такое дельта, что все иксы, которые лежат вот здесь, попадают вот сюда.\n",
      "\n",
      "176\n",
      "Какое бы маленькое эпсилон будут не взяли, существует такое \u001b[1mдельта\u001b[0m, что все иксы, которые лежат вот здесь, попадают вот сюда.\n",
      "\n",
      "177\n",
      "Какое бы маленькое эпсилон будут не взяли, существует такое дельта, что все \u001b[1mиксы\u001b[0m, которые лежат вот здесь, попадают вот сюда.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # функция для красивого печатания\n",
    "# for seg_id in segments:\n",
    "#     start, end = segments[seg_id].char_start, segments[seg_id].char_end\n",
    "#     sent = segments[seg_id].sentence\n",
    "#     print(seg_id)\n",
    "#     print(sent[:start] + '\\033[1m' + sent[start:end] + '\\033[0m' + sent[end:])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "399efad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61b9cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db17656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac821c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class SegmentInfo:\n",
    "    seg_text: str\n",
    "    available_search_area: str\n",
    "    parent_id: str\n",
    "    prev_char_count: int = 0\n",
    "    char_start: int = 0\n",
    "    char_end: int = 0\n",
    "    features: str = None\n",
    "    govern_model: str = None\n",
    "    sentence: str = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a32dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBHandler:\n",
    "    conn = None\n",
    "    cur = None\n",
    "\n",
    "    def __init__(self, db_path):\n",
    "        self.conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "        self.cur = self.conn.cursor()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.conn.close()\n",
    "\n",
    "    def add_text(self, text_name):\n",
    "        \n",
    "        self.cur.execute('''INSERT or IGNORE INTO texts (name)\n",
    "                            VALUES (?)\n",
    "                            RETURNING id''', (text_name,))\n",
    "    \n",
    "        added_id = self.cur.fetchone()\n",
    "        self.conn.commit()\n",
    "        \n",
    "        if not added_id:\n",
    "            self.cur.execute('''SELECT id\n",
    "                                FROM texts\n",
    "                                WHERE name = (?)''', (text_name, ))\n",
    "            added_id = self.cur.fetchone()\n",
    "        return added_id[0]\n",
    "        \n",
    "    def add_sent(self, text_id, sent, lemmatized, sent_count):\n",
    "        \n",
    "        self.cur.execute('''INSERT INTO sents (text_id, sent, lemmatized, pos_in_text)\n",
    "                            VALUES (?, ?, ?, ?)\n",
    "                            RETURNING id''', (text_id, sent, lemmatized, sent_count))\n",
    "        \n",
    "        added_id = self.cur.fetchone()\n",
    "        self.conn.commit()\n",
    "        return added_id[0]\n",
    "    \n",
    "    \n",
    "    def add_tokens(self, tokens_info): \n",
    "        \":param tokens_info: list of tuples (sent_id, word_in_sent, token, char_start, char_end)\"\n",
    "        \n",
    "        self.cur.executemany('''INSERT INTO tokens (sent_id, word_in_sent, token, char_start, char_end)\n",
    "                                VALUES (?, ?, ?, ?, ?)''', tokens_info)\n",
    "        \n",
    "        self.conn.commit()\n",
    "        \n",
    "        self.cur.execute('''SELECT id \n",
    "                            FROM tokens\n",
    "                            WHERE sent_id = (?)\n",
    "                            ORDER BY word_in_sent''', (tokens_info[0][0],))\n",
    "        added_ids = self.cur.fetchall()\n",
    "        return [token_id[0] for token_id in added_ids]\n",
    "    \n",
    "    \n",
    "    def add_lemmas(self, lemmas):\n",
    "        lemmas = [(el,) for el in lemmas]\n",
    "        self.cur.executemany('''INSERT or IGNORE\n",
    "                            INTO lemmas (name)\n",
    "                            VALUES (?)''', lemmas)\n",
    "        self.conn.commit()\n",
    "        \n",
    "        lemma_ids = []\n",
    "        for lemma in lemmas:\n",
    "            self.cur.execute('''SELECT id\n",
    "                                FROM lemmas\n",
    "                                WHERE name = ?''', lemma)\n",
    "            lemma_ids.append(self.cur.fetchone()[0])\n",
    "        return lemma_ids\n",
    "    \n",
    "    \n",
    "    def add_grammar_annot(self, token_grammar_info):\n",
    "        \":param token_grammar_info: [(token_id, pos, deprel, head_id, lemma)]\"\n",
    "        \n",
    "        self.cur.executemany('''INSERT INTO grammar_annotation (token_id, lemma_id, pos_id, deprel_id, head_id)\n",
    "                                SELECT ?, \n",
    "                                        lemmas.id, \n",
    "                                        (SELECT pos.id FROM pos WHERE pos.name = ?),\n",
    "                                        (SELECT deprels.id FROM deprels WHERE deprels.name = ?),\n",
    "                                        ?\n",
    "                                FROM lemmas\n",
    "                                WHERE lemmas.name = ?''', token_grammar_info)\n",
    "        self.conn.commit()\n",
    "        \n",
    "    def add_math_tags(self, tags):\n",
    "        tags = [(el,) for el in tags]\n",
    "        self.cur.executemany('''INSERT or IGNORE\n",
    "                                INTO math_tags (feature)\n",
    "                                VALUES (?)''', tags)\n",
    "        self.conn.commit()\n",
    "        \n",
    "    def add_govern_models(self, models):\n",
    "        models = [(el,) for el in models]\n",
    "        self.cur.executemany('''INSERT or IGNORE\n",
    "                                INTO govern_models (model)\n",
    "                                VALUES (?)''', models)\n",
    "        self.conn.commit()\n",
    "        \n",
    "    def get_sent_id(self, sentence):\n",
    "        self.cur.execute('''SELECT id\n",
    "                            FROM sents\n",
    "                            WHERE sents.sent = (?)''', (sentence,))\n",
    "#         print(sentence)\n",
    "        return self.cur.fetchone()[0]\n",
    "    \n",
    "    \n",
    "    def add_math_annotation(self, annotation):\n",
    "        \"\"\":param annotation: [{\n",
    "            'sentence': str, \n",
    "            'segment_text': str,\n",
    "            'char_start': int, \n",
    "            'char_end': int,\n",
    "            'annot': str, \n",
    "            'govern_model': str}\n",
    "            }]\"\"\"\n",
    "        \n",
    "        annot = list(set([an['annot'] for an in annotation]))\n",
    "        self.add_math_tags(annot)\n",
    "        govern_models = list(set([an['govern_model'] for an in annotation if an['govern_model']]))\n",
    "        self.add_govern_models(govern_models)\n",
    "        \n",
    "        sent_ids = {}\n",
    "        for annot in annotation:\n",
    "            if not sent_ids.get(annot['sentence']):\n",
    "                sent_ids[annot['sentence']] = self.get_sent_id(annot['sentence'])\n",
    "            annot['sentence_id'] = sent_ids[annot['sentence']]\n",
    "            \n",
    "        self.cur.executemany('''INSERT INTO math_annotation (feature_id, govern_model_id, id_token_start, id_token_end)\n",
    "                                SELECT math_tags.id, \n",
    "                                (SELECT govern_models.id FROM govern_models WHERE govern_models.model = :govern_model),\n",
    "                                (SELECT tokens.id FROM tokens \n",
    "                                WHERE tokens.char_start >= :char_start AND tokens.char_end <= :char_end AND tokens.sent_id = :sentence_id\n",
    "                                ORDER BY tokens.word_in_sent \n",
    "                                LIMIT 1),\n",
    "                                (SELECT tokens.id FROM tokens \n",
    "                                WHERE tokens.char_start >= :char_start AND tokens.char_end <= :char_end AND tokens.sent_id = :sentence_id\n",
    "                                ORDER BY tokens.word_in_sent DESC\n",
    "                                LIMIT 1)\n",
    "                            FROM math_tags\n",
    "                            WHERE math_tags.feature = :annot''', annotation)\n",
    "        self.conn.commit()\n",
    "        \n",
    "    def change_text_status(self, text_id, new_status):\n",
    "        self.cur.execute('''UPDATE texts\n",
    "                            SET status_id = (?)\n",
    "                            WHERE id = (?)''', (new_status, text_id))\n",
    "        self.conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc7a5a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XML2Database:\n",
    "    \n",
    "    def __init__(self, xml_file, database_path, textname=None):\n",
    "        \n",
    "        with open(xml_filename, 'r', encoding='utf-8') as f:\n",
    "            xml_file = f.read()\n",
    "        self.bs_data = BeautifulSoup(xml_file, \"xml\")\n",
    "        self.db = DBHandler(db_path)\n",
    "        self.nlp = spacy.load(\"ru_core_news_sm\")\n",
    "        \n",
    "        if not textname:\n",
    "            textname = re.search(r'Texts\\/(.*?)\\.txt', \n",
    "                                 self.bs_data.find('textfile').get_text()).group(1)\n",
    "        self.text_id = self.db.add_text(textname)\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def parse_sentence_info_(analysed_sent):\n",
    "        parsed = {\n",
    "            'tokens': [], \n",
    "            'lemmas': [], \n",
    "            'poses': [], \n",
    "            'deprels': [], \n",
    "            'head_id_in_sent': [], \n",
    "            'full_sentence': analysed_sent.text,\n",
    "        }\n",
    "        first_word_in_sent = 0\n",
    "\n",
    "        for t in analysed_sent:\n",
    "            parsed['tokens'].append(t.text)\n",
    "            parsed['lemmas'].append(t.lemma_)\n",
    "            parsed['poses'].append(t.pos_)\n",
    "            parsed['deprels'].append(t.dep_.lower())\n",
    "\n",
    "            if t.is_sent_start:\n",
    "                first_word_in_sent = t.i\n",
    "\n",
    "            parsed['head_id_in_sent'].append(t.head.i - first_word_in_sent)\n",
    "        parsed['lemmatized_sent'] = ' '.join(parsed['lemmas'])\n",
    "        return parsed\n",
    "    \n",
    "    @staticmethod\n",
    "    def accum_token_info_(parsed, sent_id):\n",
    "        n_tokens = len(parsed['tokens'])\n",
    "        token_info = []\n",
    "        sent_string = parsed['full_sentence']\n",
    "        prev_chars_count = 0\n",
    "        for i in range(n_tokens):\n",
    "            token = parsed['tokens'][i]\n",
    "            m = re.search(re.escape(token), sent_string)\n",
    "            char_start, char_end = m.start(), m.end()\n",
    "\n",
    "            token_info.append((sent_id, i, parsed['tokens'][i], prev_chars_count + char_start, prev_chars_count + char_end))\n",
    "            prev_chars_count += char_end\n",
    "            sent_string = sent_string[char_end:]  # обрезаем строку, чтобы короткие слова не искались в начале строки\n",
    "        return token_info\n",
    "    \n",
    "    @staticmethod\n",
    "    def accum_grammar_info_(parsed, added_tokens):\n",
    "        n_tokens = len(parsed['tokens'])\n",
    "        head_ids = [added_tokens[hi] for hi in parsed['head_id_in_sent']]\n",
    "        token_grammar_info = [\n",
    "        (\n",
    "            added_tokens[i], \n",
    "            parsed['poses'][i],\n",
    "            parsed['deprels'][i], \n",
    "            head_ids[i],\n",
    "            parsed['lemmas'][i]\n",
    "        ) for i in range(n_tokens)\n",
    "        ]\n",
    "        return token_grammar_info\n",
    "        \n",
    "    def extract_sentences(self):\n",
    "        full_text = self.bs_data.find('body').get_text().strip()\n",
    "        sent_count = 0\n",
    "        for sent in tqdm(self.nlp(full_text).sents):\n",
    "            sent_count += 1\n",
    "            parsed_sentence = XML2Database.parse_sentence_info_(sent)\n",
    "            \n",
    "            lemmas = parsed_sentence['lemmas']\n",
    "            added_lemmas = self.db.add_lemmas(lemmas)\n",
    "            \n",
    "            sent_text = parsed_sentence['full_sentence']\n",
    "            sent_lemmatized = parsed_sentence['lemmatized_sent']\n",
    "            sent_id = self.db.add_sent(self.text_id, sent_text, sent_lemmatized, sent_count)\n",
    "            \n",
    "            tokens_info = XML2Database.accum_token_info_(parsed_sentence, sent_id)\n",
    "            added_tokens = self.db.add_tokens(tokens_info)\n",
    "            \n",
    "            grammar_info = XML2Database.accum_grammar_info_(parsed_sentence, added_tokens)\n",
    "            self.db.add_grammar_annot(grammar_info)\n",
    "        self.db.change_text_status(self.text_id, 2)\n",
    "#         print('all sentences added to dabase')\n",
    "            \n",
    "    @staticmethod\n",
    "    def parse_annotation_segments_(bs_data):\n",
    "        segments = {}\n",
    "        for s in bs_data.find('body').find_all('segment', recursive=True):\n",
    "            parent_id = s.parent.get('id')\n",
    "            segments[s.get('id')] = SegmentInfo(s.get_text().strip(), s.get_text().strip(), parent_id)\n",
    "        for seg_id in segments:\n",
    "            segment_xml = bs_data.find('segment', {'id': seg_id})\n",
    "            parent_id = segments[seg_id].parent_id\n",
    "            \n",
    "            if not parent_id:  # то есть, что это сегмент с предложением \n",
    "                segments[seg_id].sentence = segments[seg_id].seg_text\n",
    "            else:\n",
    "\n",
    "                search_area = segments[parent_id].available_search_area\n",
    "                m = re.search('(?<![A-я])' + re.escape(segments[seg_id].seg_text) + '(?![A-я])', search_area)\n",
    "                start, end = m.start(), m.end()\n",
    "                segments[seg_id].char_start = segments[parent_id].char_start + segments[parent_id].prev_char_count + start\n",
    "                segments[seg_id].char_end = segments[parent_id].char_start + segments[parent_id].prev_char_count + end\n",
    "                segments[seg_id].sentence = segments[parent_id].sentence\n",
    "                \n",
    "                if segment_xml.get('features'):\n",
    "                    segments[seg_id].features = segment_xml.get('features')\n",
    "                if segment_xml.get('comment'):\n",
    "                    segments[seg_id].govern_model = segment_xml.get('comment')\n",
    "\n",
    "                segments[parent_id].available_search_area = segments[parent_id].available_search_area[end:]\n",
    "                segments[parent_id].prev_char_count += m.end()\n",
    "        return segments\n",
    "            \n",
    "    def extract_math_annotation(self):\n",
    "        annot_segments = []\n",
    "        segments = XML2Database.parse_annotation_segments_(self.bs_data)\n",
    "        for seg_id in segments:\n",
    "            if segments[seg_id].features:\n",
    "                sentences = list(self.nlp(segments[seg_id].sentence).sents)\n",
    "                sentences_starts = [s.start_char for s in sentences]\n",
    "                segment_sent = sentences_starts.index([s for s in sentences_starts if s <= segments[seg_id].char_start][-1])\n",
    "                new_char_start = segments[seg_id].char_start - sentences_starts[segment_sent]\n",
    "                new_char_end = segments[seg_id].char_end - sentences_starts[segment_sent]\n",
    "                annot_segments.append({\n",
    "                    'sentence': sentences[segment_sent].text,\n",
    "                    'segment_text': segments[seg_id].seg_text,\n",
    "                    'char_start': new_char_start,\n",
    "                    'char_end': new_char_end,\n",
    "                    'annot': segments[seg_id].features,\n",
    "                    'govern_model': segments[seg_id].govern_model\n",
    "                })\n",
    "                bolded_print(sentences[segment_sent].text, (new_char_start, new_char_end))\n",
    "        self.db.add_math_annotation(annot_segments)\n",
    "        print('all math annotation added to database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f96b2395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "207it [01:53,  1.82it/s]\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "near \".\": syntax error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m db_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmath_corpus_database.db\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m xml_to_db \u001b[38;5;241m=\u001b[39m XML2Database(xml_filename, db_path)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mxml_to_db\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36mXML2Database.extract_sentences\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     90\u001b[0m     grammar_info \u001b[38;5;241m=\u001b[39m XML2Database\u001b[38;5;241m.\u001b[39maccum_grammar_info_(parsed_sentence, added_tokens)\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb\u001b[38;5;241m.\u001b[39madd_grammar_annot(grammar_info)\n\u001b[1;32m---> 92\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchange_text_status\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36mDBHandler.change_text_status\u001b[1;34m(self, text_id, new_status)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchange_text_status\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_id, new_status):\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'''\u001b[39;49m\u001b[38;5;124;43mUPDATE texts\u001b[39;49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;43m                        SET texts.status_id = ?\u001b[39;49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;43m                        WHERE texts.id = ?\u001b[39;49m\u001b[38;5;124;43m'''\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_status\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconn\u001b[38;5;241m.\u001b[39mcommit()\n",
      "\u001b[1;31mOperationalError\u001b[0m: near \".\": syntax error"
     ]
    }
   ],
   "source": [
    "xml_filename = 'Производная.Начало.xml'\n",
    "db_path = 'math_corpus_database.db'\n",
    "\n",
    "xml_to_db = XML2Database(xml_filename, db_path)\n",
    "xml_to_db.extract_sentences()\n",
    "# xml_to_db.extract_math_annotation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64e2c7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBHandler(db_path)\n",
    "db.change_text_status(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34156539",
   "metadata": {},
   "outputs": [],
   "source": [
    "del db\n",
    "del xml_to_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306b2efb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
